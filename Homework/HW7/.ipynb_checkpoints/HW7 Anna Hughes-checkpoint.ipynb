{
 "metadata": {
  "name": "",
  "signature": "sha256:2ff985e04d3de3b1c8e7361e3f54a046abaf5a293644b42b6dacc4119ffbf5ab"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "df=pd.read_csv('wine.csv',header=None)\n",
      "target=df[0]\n",
      "feature_list=[1,2,3,4,5,6,7,8,9,10,11,13]\n",
      "features=df[feature_list]\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "      <th>3</th>\n",
        "      <th>4</th>\n",
        "      <th>5</th>\n",
        "      <th>6</th>\n",
        "      <th>7</th>\n",
        "      <th>8</th>\n",
        "      <th>9</th>\n",
        "      <th>10</th>\n",
        "      <th>11</th>\n",
        "      <th>12</th>\n",
        "      <th>13</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> 14.23</td>\n",
        "      <td> 1.71</td>\n",
        "      <td> 2.43</td>\n",
        "      <td> 15.6</td>\n",
        "      <td> 127</td>\n",
        "      <td> 2.80</td>\n",
        "      <td> 3.06</td>\n",
        "      <td> 0.28</td>\n",
        "      <td> 2.29</td>\n",
        "      <td> 5.64</td>\n",
        "      <td> 1.04</td>\n",
        "      <td> 3.92</td>\n",
        "      <td> 1065</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 1</td>\n",
        "      <td> 13.20</td>\n",
        "      <td> 1.78</td>\n",
        "      <td> 2.14</td>\n",
        "      <td> 11.2</td>\n",
        "      <td> 100</td>\n",
        "      <td> 2.65</td>\n",
        "      <td> 2.76</td>\n",
        "      <td> 0.26</td>\n",
        "      <td> 1.28</td>\n",
        "      <td> 4.38</td>\n",
        "      <td> 1.05</td>\n",
        "      <td> 3.40</td>\n",
        "      <td> 1050</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 1</td>\n",
        "      <td> 13.16</td>\n",
        "      <td> 2.36</td>\n",
        "      <td> 2.67</td>\n",
        "      <td> 18.6</td>\n",
        "      <td> 101</td>\n",
        "      <td> 2.80</td>\n",
        "      <td> 3.24</td>\n",
        "      <td> 0.30</td>\n",
        "      <td> 2.81</td>\n",
        "      <td> 5.68</td>\n",
        "      <td> 1.03</td>\n",
        "      <td> 3.17</td>\n",
        "      <td> 1185</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1</td>\n",
        "      <td> 14.37</td>\n",
        "      <td> 1.95</td>\n",
        "      <td> 2.50</td>\n",
        "      <td> 16.8</td>\n",
        "      <td> 113</td>\n",
        "      <td> 3.85</td>\n",
        "      <td> 3.49</td>\n",
        "      <td> 0.24</td>\n",
        "      <td> 2.18</td>\n",
        "      <td> 7.80</td>\n",
        "      <td> 0.86</td>\n",
        "      <td> 3.45</td>\n",
        "      <td> 1480</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 1</td>\n",
        "      <td> 13.24</td>\n",
        "      <td> 2.59</td>\n",
        "      <td> 2.87</td>\n",
        "      <td> 21.0</td>\n",
        "      <td> 118</td>\n",
        "      <td> 2.80</td>\n",
        "      <td> 2.69</td>\n",
        "      <td> 0.39</td>\n",
        "      <td> 1.82</td>\n",
        "      <td> 4.32</td>\n",
        "      <td> 1.04</td>\n",
        "      <td> 2.93</td>\n",
        "      <td>  735</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "   0      1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
        "0   1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
        "1   1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
        "2   1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
        "3   1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
        "4   1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
        "\n",
        "     13  \n",
        "0  1065  \n",
        "1  1050  \n",
        "2  1185  \n",
        "3  1480  \n",
        "4   735  "
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "1. Classify the raw data using a linear SVM. Do you need to perform several binary classifications or does scikit-learn support multi-class classification with SVMs?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "def cross_validate(X, y, classifier, k_fold) :\n",
      "\n",
      "    # derive a set of (random) training and testing indices\n",
      "    k_fold_indices = KFold( len(X), n_folds=k_fold,\n",
      "                            shuffle=True,\n",
      "                           random_state=0)\n",
      "\n",
      "    k_score_total = 0\n",
      "    # for each training and testing slices run the classifier, and score the results\n",
      "    for train_slice, test_slice in k_fold_indices :\n",
      "\n",
      "        model = classifier(X[ train_slice  ],\n",
      "                         y[ train_slice  ])\n",
      "\n",
      "        k_score = model.score(X[ test_slice ],\n",
      "                              y[ test_slice ])\n",
      "        k_score_total += k_score\n",
      "\n",
      "    # return the average accuracy\n",
      "    return k_score_total/k_fold"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "model_SVM=SVC(C=1)\n",
      "model_LR=LogisticRegression(C=1.0)\n",
      "#a=model.fit(features,target).predict(features)\n",
      "CV1=cross_validate(features.values, target, model_SVM.fit, 20)\n",
      "CV2=cross_validate(features.values, target, model_LR.fit, 20)\n",
      "print CV1, CV2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.44375 0.954861111111\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The cross-validation score is very low, much lower than for other classifiers "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "3. Preprocess the data with a normalization step"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import preprocessing\n",
      "features_n=preprocessing.scale(features)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "4. Repeat the classification performed in step 1 using a linear SVM and crossvalidate the result. Is it better or worse?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scaler = preprocessing.StandardScaler().fit(features)\n",
      "\n",
      "CV3=cross_validate(features_n,target,model_SVM.fit, 20)\n",
      "\n",
      "print CV3\n",
      "\n",
      "features_n1=scaler.transform(features)\n",
      "\n",
      "CV4=cross_validate(features_n1,target,model_SVM.fit, 20)\n",
      "\n",
      "print CV4\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.977777777778\n",
        "0.977777777778\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result is better with crossvalidation score 0.98"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "5. Implement a pipeline that comprises: a preprocessing and classification step"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.pipeline import Pipeline\n",
      "clf = Pipeline([('norm', scaler), ('svm',SVC())])\n",
      "CV5=cross_validate(features.values,target,clf.fit, 20)\n",
      "\n",
      "print CV5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.977777777778\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "the results for a pipeline is the same as expected"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "6. Try varying the value of C or the type of kernel. Do you get better results?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "6.1. Varying the value of C"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "C is a penalty parameter of the error term. \n",
      "Basically, the high value of C means we have high penalty for nonseparable points and we can overfit. (high complexity of decision rule)\n",
      "\n",
      "The low values for C means low penalty and we can underfit. (high frequency of error)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(1,4):\n",
      "    c=i\n",
      "    model_SVM=SVC(C=c)\n",
      "    CV=cross_validate(features_n,target,model_SVM.fit, 20)\n",
      "    print CV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.977777777778\n",
        "0.983333333333\n",
        "0.977777777778"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "no results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kernel_list=['linear', 'poly', 'rbf', 'sigmoid']\n",
      "for ker in kernel_list:\n",
      "    model_SVM=SVC(C=1, kernel=ker)\n",
      "    CV=cross_validate(features_n,target,model_SVM.fit, 20)\n",
      "    print ker,CV\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "linear 0.955555555556\n",
        "poly 0.955555555556\n",
        "rbf"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.977777777778\n",
        "sigmoid 0.398611111111\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "the default kernel ('rbf') gives the best results"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "7. Feed your pipeline classifier to the grid search"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm, grid_search, datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parameters = {\n",
      "    'kernel':['linear', 'poly','rbf','sigmoid'], \n",
      "    'C':[0.1,0.2,0.3,0.5,0.7,1], \n",
      "    'gamma':[0,0.001, 0.01, 0.1, 0.3,1,100]}\n",
      "svc=svm.SVC()\n",
      "gs = grid_search.GridSearchCV(svc, parameters, refit=False, cv=3)\n",
      "\n",
      "gs.fit(features_n, target)\n",
      "print gs.best_params_, gs.best_score_\n",
      "\n",
      "svc1=svm.SVC(kernel='rbf',C=0.5,gamma=0)\n",
      "CV=cross_validate(features_n, target, svc1.fit, 20)\n",
      "print CV\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'kernel': 'rbf', 'C': 0.5, 'gamma': 0} 0.977528089888\n",
        "0.977777777778\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The most accurate clasifier (out of the C, gamma and kernels I made a grid search through) showathe same results as my initial one. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}